---
title: "DSA 5013-001 Homework 6"
author: "Group 7: Zachary White, Azadeh Gilanpour, Precious Jatau"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: word_document
geometry: margin = 0.8in
documentclass: article
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

These packages were used to find the results in Homework 6

```{r}
library(caret) #train conrtol, #?????
library(ipflasso)
```

#Problem 1: Predicting Violent Crime Rate

Given real-world data relating to various communities and their socio-demographics, law enforcement details, and crime statistics, your  goal is to predict the community-level per capita violent crimes. The target variable is continuous and you may use any techniques at your disposal to produce a highly predictive model.

###Problem 1a

Explore the provided data, conduct any desired data preparation. Summarize your approach.

The Training set and test set are imported as crimeTrain and crimeTest.

```{r, echo = TRUE, results='hide'}
crimeTrain = read.csv("C:\\Users\\zackw\\Documents\\Classes\\Intelligent Data Analytics\\HW\\HW6\\crimeTrain.csv")
crimeTest = read.csv("C:\\Users\\zackw\\Documents\\Classes\\Intelligent Data Analytics\\HW\\HW6\\crimeTest.csv")
```

There were three criteria for data that was removed: Significantly High Missingness, Too Unique Values, and High Corresposdence

Columns with an average of missing values greater that 80% were removed. The columns that were removed were: FTPolicePerPop, FTPoliceFieldPerPop, RacialMatchCommPol, PctPolicWhite, PctPolicBlack, PctPolicHisp, PctPolicAsian, PctPolicMinor, OfficAssgnDrugUnits, PolicAveOTWorked, PolicCars, PolicOperBudg, PctPolicOnPatr, GangUnitDeploy, and PolicBudgPerPop.

```{r, echo = TRUE, results='hide'}
myfun = function(x) mean(is.na(x))

missing = apply(crimeTrain,2,myfun)
missInd =  which(missing > 0.8);

# delete all variables with missingnes greater than .8
crimeTrain = crimeTrain[,-missInd]
```

Columns that were too unique were removed since the values they had could be used as categorical and did not bring much new information due to how unique the values were. When One-Hot Encoding occured with the factor parameter, the new parameters would all have near zero variance. The values that were removed were X, county, state, and communityname.

```{r, echo =TRUE, results='hide'}
# remove column id
crimeTrain  = crimeTrain[, - which(colnames(crimeTrain) == "X")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "county")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "communityname")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "state")]
```

Columns that gave very similar information or had high correspondence were removed. 

Columns that gave very similar information were parameters that showed information that were already given. For example, whitePerCap, blackPerCap, indianPerCap, AsianPerCap, and otherPerCap is repetitive since we know the ratio of races and we know the per Capitol income of each community. 

Another example is NumImmig, PctImmigRecent, PctImmigRec5, PctImmigRec8, PctImmigRec10, PctRecentImmig, PctRecImmig5, and PctRecImmig8 show number of immigrants in total, within 5,8, and 10 years. There is a lot of intersection of these parameters and PctRecImmig10 would suffice for all of these.

```{r, echo =TRUE, results='hide'}
# divide age percentage to 12 - 29, > 65
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "agePct16t24")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "agePct12t21")]

# remove features
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "numbUrban")]

# cor(crimeTrain[,c("pctWWage","pctWFarmSelf","pctWInvInc")])
# not highly correlated, keep all

# cor(crimeTrain[,c("pctWSocSec","pctWPubAsst")])
# not highly correlated keep all

crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "whitePerCap")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "blackPerCap")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "indianPerCap")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "AsianPerCap")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "OtherPerCap")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "HispPerCap")]

crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "NumUnderPov")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctLess9thGrade")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctEmploy")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctOccupManu")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctEmplProfServ")]

cor(crimeTrain[,c("MalePctDivorce","MalePctNevMarr")])
# low correlation, keep both
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "TotalPctDiv")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PersPerFam")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctKids2Par")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctYoungKids2Par")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctTeen2Par")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctWorkMomYoungKids")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "NumIlleg")]

# keep PctRecImmig10
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "NumImmig")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctImmigRecent")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctImmigRec5")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctImmigRec8")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctImmigRec10")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctRecentImmig")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctRecImmig5")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctRecImmig8")]

crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctSpeakEnglOnly")]

# keep PctLargHouseOccup
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctLargHouseFam")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PersPerOccupHous")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PersPerOwnOccHous")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PersPerRentOccHous")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctPersOwnOccup")]

# Keep PctPersDenseHous
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctHousLess3BR")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "MedNumBR")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "HousVacant")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctHousOccup")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctHousOwnOcc")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctVacantBoarded")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctVacMore6Mos")]

crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "MedYrHousBuilt")]

# keep MedRentPctHousInc
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctWOFullPlumb")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "OwnOccLowQuart")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "OwnOccMedVal")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "OwnOccHiQuart")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "RentLowQ")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "RentMedian")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "RentHighQ")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "MedRent")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "MedOwnCostPctInc")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "MedOwnCostPctIncNoMtg")]

# Keep PctSameState85
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctForeignBorn")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctBornSameState")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctSameHouse85")]
crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "PctSameCity85")]

crimeTrain = crimeTrain[, - which(colnames(crimeTrain) == "LandArea")]


usedVar = colnames(crimeTrain)
usedVar = usedVar[-which((usedVar) == "ViolentCrimesPerPop")]

ind = numeric(length(usedVar));

for (i in 1:length(usedVar))
{
  ind[i] =  which(colnames(crimeTest) == usedVar[i])  
}


crimeTest = crimeTest[,ind]
```

At the end we have 42 parameters in crimeTrain. Those parameters are the following.

```{r}
colnames(crimeTrain)
```

###Problem 1b

Using cross-validation of your choice, perform PLS, ridge regression, lasso, elastic net, and SVM-regression to tune the associated hyper-parameters and estimate the generalizable error for a model to predict the community-level per capita violent crimes. Provide a chart summarizing the hyper-parameter search for each approach.

The results are shown in the table.

Model Description     | R Package | Tuned Hyper-Parameters | CV RMSE | CV R2
--------------------- | --------- | ---------------------- | ------- | -----
Partial Least Squares |   caret with pls  |     ncomp = 8          |  0.1377 |  0.6602
SVM (Linear)                   |   caret with svmLinear    |          Cost = .1          |    0.1397 |  0.6560
Ridge Regression      | ipflasso      |      lambda = 0.0192 | 0.1342  | 0.6742
Lasso                 | ipflasso      | lambda = 0.000875 | 0.1336   | 0.6770
Elastic Net           | caret with enet      | alpha = 0.525, lambda = 0.0001 | 0.1337 | 0.6766

####Cross Validation

The Cross Validation technique used for all Models is a repeated k-fold Cross Validation, where k = 10 and it is repeated 10 times.

```{r}
ctrl = trainControl(method="repeatedcv", number = 10, repeats = 10)
```

####Partial Least Squares

The caret package is used to train pls.fit with a tuneLength of 42.

```{r}
pls.fit = train(ViolentCrimesPerPop ~ ., data = crimeTrain, method = "pls",trControl=ctrl, tuneLength = 42)
```

```{r}
plot(pls.fit)
```

```{r}
pls.fit$bestTune
```

```{r}
pls.fit = train(ViolentCrimesPerPop ~ ., data = crimeTrain, method = "pls",trControl=ctrl, tuneGrid = expand.grid(ncomp = pls.fit$bestTune))
```

```{r}
pls.fit
```

```{r}
pls.predict = predict(pls.fit,crimeTrain)
ggplot(data = data.frame(obs = crimeTrain$ViolentCrimesPerPop, pred = pls.predict )) + geom_point(aes(x = obs, y = pred))
```

####Support Vector Machine

```{r, echo = TRUE, eval=FALSE}
grid2 <- expand.grid(C = seq(0,2,length=15)) #runtime is long

# using a linear kernel
svm.lm <- train(data=crimeTrainNoState, ViolentCrimesPerPop ~ ., method = "svmLinear",
    preProc = c("center","scale"), trControl=ctrl ,tuneGrid = grid2)
```

PLOT HERE

```{r}
svm.lm <- train(data=crimeTrain, 
                   ViolentCrimesPerPop ~ .,
                   method = "svmLinear",   # linear kernel
                   preProc = c("center","scale"),  # Center and scale data
                   trControl=ctrl ,tuneGrid = expand.grid(C = .1))
```

```{r}
svm.lm
```

```{r}
x = data.matrix(crimeTrain[,-which(colnames(crimeTrain) == "ViolentCrimesPerPop")])
y = data.matrix(crimeTrain[,which(colnames(crimeTrain) == "ViolentCrimesPerPop")]) 
```

```{r}
# bestTune c = .1,
svmLm.predict = predict(svm.lm,x)
```

```{r}
ggplot(data = data.frame(obs = crimeTrain$ViolentCrimesPerPop, pred = svmLm.predict )) + geom_point(aes(x = obs, y = pred))
```

```{r}
svmLm.perf = defaultSummary(data.frame(pred = svmLm.predict, obs = y))
```

####Ridge Regression

```{r}
## ridge regression
ridge.cv = cvr.glmnet(x,y, family = "gaussian", alpha = 0, nfolds = 10, ncv = 10)
lambdaOpt = ridge.cv$lambda[which.min(ridge.cv$cvm)]
```

```{r}
# plot for tuning
ggplot(data = data.frame(MAE = ridge.cv$cvm, lambda = ridge.cv$lambda), aes(x = lambda, y = MAE)) + geom_point()
```

```{r}
# fit model
ridge.model = glmnet(x = x,y = y, lambda = lambdaOpt, family = "gaussian", alpha = 0)
summary(ridge.model)
```

```{r}
# make predictions on training data
ridge.predict = predict.glmnet(object = ridge.model, newx = x)
```

```{r}
# compare training and test
observedPredicted.df = data.frame(Observed = crimeTrain$ViolentCrimesPerPop,Predicted = ridge.predict) 
colnames(observedPredicted.df) = c("obs", "pred")
ggplot(data = observedPredicted.df) + geom_point(aes(x = obs, y = pred))
```

```{r}
# rmse, Rsquared, MAE
ridge.perf = defaultSummary(observedPredicted.df)
ridge.perf
```

####Lasso

```{r, echo = TRUE, results='hide'}
lasso.cv = cvr.glmnet(x,y, family = "gaussian", alpha = 1, nfolds = 10, ncv = 10)
```

```{r}
lambdaOptLasso = lasso.cv$lambda[which.min(lasso.cv$cvm)]
```

```{r}
lambdaOptLasso
```

```{r}
# plot for tuning
ggplot(data = data.frame(MAE = lasso.cv$cvm, lambda = lasso.cv$lambda), aes(x = lambda, y = MAE)) + geom_point()
```

```{r}
# fit model
lasso.model = glmnet(x = x,y = y, lambda = lambdaOptLasso, family = "gaussian", alpha = 1)
summary(lasso.model)
```

```{r}
# make predictions on training data
lasso.predict = predict.glmnet(object = lasso.model, newx = x)
```

```{r}
# compare training and test
observedPredictedLasso.df = data.frame(Observed = crimeTrain$ViolentCrimesPerPop,Predicted = lasso.predict) 
colnames(observedPredictedLasso.df) = c("obs", "pred")
```

```{r}
# observed vs predicted for lasso
ggplot(data = observedPredictedLasso.df) + geom_point(aes(x = obs, y = pred))
```

```{r}
# rmse, r_squared, MAE
lasso.perf = defaultSummary(observedPredictedLasso.df)
lasso.perf
```

####Elastic Net

```{r, echo = TRUE, results='hide'}
# did sparse search, best tune was fraction = .525, lambda = 1e-04
alphaGrid = expand.grid(fraction = seq(0.2,0.8,length.out = 100), lambda = seq(0,.1,length.out = 100))
eNet.model = train(data = crimeTrain, ViolentCrimesPerPop ~ ., method = "enet", 
                   trControl = ctrl)#, tuneGrid = alphaGrid) 
```

```{r}
# cross validation
plot(eNet.model) # cross validation
```

```{r}
eNet.model$bestTune # show best parameters
```

```{r}
# set predictors and response
x = data.matrix(crimeTrain[,-which(colnames(crimeTrain) == "ViolentCrimesPerPop")])
y = data.matrix(crimeTrain[,which(colnames(crimeTrain) == "ViolentCrimesPerPop")])
```

```{r}
# make predictions
eNet.predict = predict(eNet.model, x)
obsPredEnet.df = data.frame(obs = crimeTrain$ViolentCrimesPerPop, pred = eNet.predict)
```

```{r}
# plot observed vs predicted
ggplot(data = obsPredEnet.df) + geom_point(aes(x = obs, y = pred))
```

```{r}
# calculate r-squared, RMSE, MAE
eNet.perf = defaultSummary(obsPredEnet.df)
eNet.perf
```

###Problem 1c

Using any regression technique or combination of techniques you prefer, predict the predict the community-level per capita violent crimes.

Since lasso, elastic Net, and Ridge Regression all have the value of lambda near 0, that we can view this as a linear model. From here, we decided to make a new linear model with a new cleaned data set. 
So crimeTrain2 is created with the raw data with all variables with the average of missing values greater than 0% removed from the data set. X, state, and community are removed too since they are either too unique or factors that will cause near zero variance.

```{r}
crimeTrain2 = read.csv("C:\\Users\\zackw\\Documents\\Classes\\Intelligent Data Analytics\\HW\\HW6\\crimeTrain.csv")
crimeTrain2 = crimeTrain2[,!(colMeans(is.na(crimeTrain)) > 0)]
crimeTrain2 = crimeTrain2[,-1:-3]
```

We made a linear model with all variables.

```{r}
linMod.fit = lm(data = crimeTrain2, ViolentCrimesPerPop ~.)
```

Next for more feature selection, we used step function with all variables to move backwards to see which variables would have the most effcient AIC and RSS. We save this as linMod.backward.

```{r, echo = TRUE, eval = FALSE, results='hide'}
linMod.backward = step(linMod.fit, direction = "backward")
```

The model step returns is ViolentCrimesPerPop ~ population + racepctblack + agePct12t29 + 
    numbUrban + pctUrban + pctWWage + pctWFarmSelf + pctWInvInc + 
    pctWRetire + whitePerCap + indianPerCap + HispPerCap + PctPopUnderPov + 
    PctLess9thGrade + PctEmploy + PctEmplManu + PctOccupManu + 
    PctOccupMgmtProf + MalePctDivorce + MalePctNevMarr + FemalePctDiv + 
    PersPerFam + PctKids2Par + PctWorkMom + PctIlleg + PctImmigRec5 + 
    PctImmigRec8 + PctImmigRec10 + PctRecentImmig + PctRecImmig8 + 
    PctRecImmig10 + PctSpeakEnglOnly + PctNotSpeakEnglWell + 
    PersPerOccupHous + PersPerRentOccHous + PctPersOwnOccup + 
    PctPersDenseHous + PctHousLess3BR + MedNumBR + PctHousOccup + 
    PctHousOwnOcc + PctVacantBoarded + PctVacMore6Mos + OwnOccLowQuart + 
    RentLowQ + MedRent + MedRentPctHousInc + MedOwnCostPctInc + 
    MedOwnCostPctIncNoMtg + NumStreet + PctForeignBorn + PctSameCity85 + 
    PctUsePubTrans
    
We use this to set an upper bound when we use the step function again for the forward direction.

```{r, echo = TRUE, eval = FALSE, results='hide'}
range = list(upper=~population + racepctblack + 
               agePct12t29 + numbUrban + pctUrban + pctWWage + pctWFarmSelf + 
               pctWInvInc + pctWRetire + whitePerCap + indianPerCap + HispPerCap + 
               PctPopUnderPov + PctLess9thGrade + PctEmploy + PctEmplManu + 
               PctOccupManu + PctOccupMgmtProf + MalePctDivorce + MalePctNevMarr + 
               FemalePctDiv + PersPerFam + PctKids2Par + PctWorkMom + PctIlleg + 
               PctImmigRec5 + PctImmigRec8 + PctImmigRec10 + PctRecentImmig + 
               PctRecImmig8 + PctRecImmig10 + PctSpeakEnglOnly + PctNotSpeakEnglWell + 
               PersPerOccupHous + PersPerRentOccHous + PctPersOwnOccup + 
               PctPersDenseHous + PctHousLess3BR + MedNumBR + PctHousOccup + 
               PctHousOwnOcc + PctVacantBoarded + PctVacMore6Mos + OwnOccLowQuart + 
               RentLowQ + MedRent + MedRentPctHousInc + MedOwnCostPctInc + 
               MedOwnCostPctIncNoMtg + NumStreet + PctForeignBorn + PctSameCity85 + 
               PctUsePubTrans, lower=~.)

linMod.forward = step(lm(data = crimeTrain2, ViolentCrimesPerPop~1),range, direction="forward")
```

In order to make a model with the fewer variables, we will be using linMod.forward.
Looking at the summary of linMod.forward, we remove the Pr(>|t|) values greater than .05. Then look at the summary of the new model and repeating the process of removing variables until all variables have Pr(>|t|) values less than .05.

By that standard we remove FemalePctDiv, MalePctDivorce, PctSpeakEnglOnly, PctForeignBorn,MalePctNevMarr,MedRentPctHousInc,PctNotSpeakEnglWell,PctUsePubTrans, and MedOwnCostPctInc. We now have a model:

lm(formula = ViolentCrimesPerPop ~ PctIlleg + PctPersDenseHous + 
    PctHousOccup + pctUrban + racepctblack + NumStreet + PctKids2Par + 
    PctWorkMom + agePct12t29 + PctEmploy + MedOwnCostPctIncNoMtg + 
    PctVacantBoarded + PctVacMore6Mos + pctWWage + pctWRetire + 
    PctPopUnderPov + RentLowQ + MedRent + whitePerCap + PctEmplManu, 
    data = crimeTrain2)
    
```{r, echo = TRUE, eval = FALSE, results='hide'}
summary(linMod.forward)
```

```{r, echo = TRUE, eval = FALSE,results='hide'}
lm(formula = ViolentCrimesPerPop ~ PctIlleg + PctPersDenseHous + 
    PctHousOccup + pctUrban + racepctblack + NumStreet + PctKids2Par + 
    PctWorkMom + agePct12t29 + PctEmploy + MedOwnCostPctIncNoMtg + 
    PctVacantBoarded + PctVacMore6Mos + pctWWage + pctWRetire + 
    PctPopUnderPov + RentLowQ + MedRent + whitePerCap + PctEmplManu, 
    data = crimeTrain2)
```

From here we did cross validation with this linear model relation. The CV technique was repeated k-fold where k = 10 and was repeated 100.

```{r, echo = TRUE, eval = FALSE,results='hide'}
ctrl.lm <- trainControl(method="repeatedcv", number=10, repeats = 100)

lm.model = train(ViolentCrimesPerPop ~ PctIlleg + PctPersDenseHous + PctHousOccup + pctUrban + racepctblack +
                   NumStreet + PctKids2Par + PctWorkMom + agePct12t29 + PctEmploy + MedOwnCostPctIncNoMtg +  
                   PctVacantBoarded + PctVacMore6Mos + pctWWage + pctWRetire +
                   PctPopUnderPov + RentLowQ + MedRent + whitePerCap + PctEmplManu, 
                 data = crimeTrain, method = "lm",trControl = ctrl.lm)

lm.model
```